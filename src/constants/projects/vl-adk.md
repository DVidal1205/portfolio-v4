VL-ADK was a resounding success at ShellHacks 2025, winning **1st place** in the NVIDIA Hack for the Future challenge, **2nd place** in the Microsoft AI for Good challenge, and **3rd place** in the Waymo Reimagining Navigation challenge. VL-ADK is our DIY take on the new frontier of Physical Agents and VLA technologies. Built on top of the Jetson Orin Nano and Jetbot framework, we allowed generalized AI models to control a physical robot.

We utilized **Google ADK** to create an autonomous fleet of agents, comprised of the Director in charge of long term planning and completion, the Observer who relayed visual and spatial information to the team, and the Pilot who had direct access to the Jetbot's control schema via tool calling. This project was fun, and truly innovative as it explored the idea of generalization for physical agents, allowing the breadth of LLMs and their tool calling capabilities the ability to control a physical robot.

We were extremely limited on resources, only having a two wheeled differential drive robot and a single camera. Given more advanced hardware and technology, we can image VL-ADK and our little Jetbot Hotdog doing some truly amazing things.

Highlights:
• **Autonomous fleet of agents** with Google ADK
• **Tool calling** for physical control
• **Hardware** built with NVIDIA Jetson Orin Nano and Jetbot platform
• **Object Detection** with YoloE models for dynamic label swapping
• **Accelerated** computing with NVIDIA RTX 5080 Graphics Card

Other Contributors:
• [Leonard Gofman](https://www.linkedin.com/in/lgofman/)
• [Luke Cullen](https://www.linkedin.com/in/luke-cullen-319701305/)
