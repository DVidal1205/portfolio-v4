AirJam is an **AI-powered music exploration tool** that let's you be a one man band, anytime, anywhere. We built this project for HackUSF 2025 after wanting to experiment with some **real-time computer vision**. By streaming frames from your webcam to a **YOLO pose detection** model via a **WebSocket**, we were able to detect hand gestures and map them to different instruments.

We implemented over **30 different instruments** via **WebAudioFonts**, spanning across a variety of different groups (percussion, strings, woodwinds, etc). We also implemented a repetition feature, so you can create a rhythmic beat for your main instrument. This project was a lot of fun, and was built in a **24-hour** Hackathon (my team and I are used to 36-hour hackathons).

Highlights:
• **Computer vision-powered** instrument control
• **Real-time hand tracking** with MediaPipe
• **YOLO object detection** for gesture recognition
• **WebAudioFonts integration** for authentic instrument sounds
• **30+ instruments** supported through gesture recognition

Other Contributors:
• [Leonard Gofman](https://www.linkedin.com/in/lgofman/)
• [Luke Cullen](https://www.linkedin.com/in/luke-cullen-319701305/)
